{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "078b03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import sys\n",
    "from os.path import abspath, join, dirname, exists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import jieba\n",
    "import random\n",
    "import csv\n",
    "import difflib\n",
    "import ast\n",
    "import json\n",
    "import nest_asyncio\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, join(abspath(dirname('__file__')), '../../../'))\n",
    "\n",
    "import application as app\n",
    "from chatgpt import gpt_utils\n",
    "from extensions import utils\n",
    "\n",
    "from models.analyze.analyze_tool import used_time\n",
    "from models.tiktok.video_common import *\n",
    "from models.nlp.common import text_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731c8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e2239fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect db 10.21.90.164 douyin2\n",
      "Connect clickhouse 10.21.90.164 sop\n",
      "Connect db 10.21.90.164 cleaner\n"
     ]
    }
   ],
   "source": [
    "dy2 = app.connect_db('dy2')\n",
    "\n",
    "db_sop = app.get_clickhouse('chsop')\n",
    "db_sop.connect()\n",
    "\n",
    "db = app.connect_db('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcce6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorynm = '运动鞋/运动穿搭'\n",
    "cat = 3000003\n",
    "subcid = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e595303",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_head = [\n",
    "        {\"role\": \"system\", \"content\": '''假设你是一个资深短视频内容分析师，熟悉短视频内容，十分严谨，擅长识别短视频文本的主题，擅长对短视频文本做归类，擅长分析短视频文本的描述主体。'''},\n",
    "    {\"role\": \"user\", \"content\": '''【需求】\n",
    "                我会先给你一段文本，请你，\n",
    "                1、先找出与{category_nm}相关的实体或产品，只要找到，就回答“此文本与{category_nm}相关，提到了与{category_nm}相关的xxx产品”并结束对话，\n",
    "                2、如果找不到与{category_nm}相关的实体或产品，请你判断整体文本是否跟“{category_nm}”相关，如果不相关请直接回答“此文本与{category_nm}无关”并简要说明理由，如果相关请回答“此文本与{category_nm}相关”并简要说明理由。'''},\n",
    "     {\"role\": \"user\", \"content\": '''【示例】\n",
    "                例子1、文本：“我喜欢在他爸手里,,我想我的亲人你说你俩忙一天,你俩吃饭了吗?这身体累垮了怎么办,?等于,赶快吃了吧,,私底下就是底下新品半条士力架,糖分减少58%,到某烟卖两种口味,香脆坚果和谷物在脖上,铃铛和小好吃不甜腻,工作没时间吃饭”，此文本与巧克力相关，因为提到了士力架巧克力的特点。\n",
    "                例子2、文本：“有白桃茉莉味,紫薯芋泥味,杨枝甘露味,竹香抹茶味,黑巧摩卡味,黑巧珍珍味,外皮糯唧唧的超q弹,白桃茉莉味尝起来很清新,里面是真的有白桃果肉,甜度也刚好,每一口都很享受,黑巧摩卡有一种很细腻的巧克力摩卡风味,搭配着糯唧唧的表皮”，此文本与巧克力无关，“巧克力摩卡风味”与巧克力相关，但整体文本在说的是口味，不是巧克力产品。\n",
    "                例子3、文本：“嘿嘿嘿,你这样拿冰糖吃呀,你都把我的棒棒糖全部烫起来了,好了好了放回去放回去,可怜的家里还有,气消米乳酸菌的,还有那个巧克力味的蛋糕,这次的气消米不一样了”，此文本与巧克力无关，因为实际上说的是巧克力味的蛋糕。'''},\n",
    "    {\"role\": \"user\", \"content\": '''【注意】\n",
    "                1、短视频内容的ocr和asr文本一般较长，而且可能有错别字，句子乱序，因此需要你先去噪。\n",
    "                2、与“{category_nm}”相关的产品可能只是在内容中简单提到，因此需要你有极强的总结分析能力。\n",
    "                3、回答时，先说结论（此文本与{category_nm}相关 或 此文本与{category_nm}无关），再简要说明理由。\n",
    "                【提问】\n",
    "                下面我会给你文本，用```包起来，请你按需求，参考示例的分析，遵守注意点，给出你的回答。\n",
    "            ```{txt}```直接给出你的回答，不要拖泥带水，不要产生幻觉。'''}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8784999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定跑gpt的范围\n",
    "# digg_count>10000 可以随意指定\n",
    "if subcid:\n",
    "    sql = f'''\n",
    "    select distinct aweme_id\n",
    "    from douyin2_cleaner.douyin_video_zl_{cat}\n",
    "    where digg_count>10000\n",
    "    and aweme_id in (select aweme_id from douyin2_cleaner.douyin_video_sub_cid_zl_{cat} where sub_cid in ({subcid}))\n",
    "    limit 10\n",
    "    '''\n",
    "    rr = dy2.query_all(sql)\n",
    "    awmids = [str(x[0]) for x in rr]\n",
    "else:\n",
    "    sql = f'''\n",
    "    select distinct aweme_id\n",
    "    from douyin2_cleaner.douyin_video_zl_{cat}\n",
    "    where digg_count>10000\n",
    "    and aweme_id in (select aweme_id from douyin2_cleaner.douyin_video_sub_cid_zl_{cat} where sub_cid<>0)\n",
    "    limit 10\n",
    "    '''\n",
    "    rr = dy2.query_all(sql)\n",
    "    awmids = [str(x[0]) for x in rr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "661f8634",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7257562810513640759',\n",
       " '7267245942326250793',\n",
       " '7255228628684852519',\n",
       " '7265139593673018636',\n",
       " '7212930080870386959',\n",
       " '7230776140158225725',\n",
       " '7200307004185873703',\n",
       " '7201072338413063483',\n",
       " '7210204920631283001',\n",
       " '7242229599843700000']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "934d3400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_type 5\n"
     ]
    }
   ],
   "source": [
    "# 对应范围取数\n",
    "\n",
    "multi_tasks = [1, 2, 3, 14]\n",
    "task_names = [type2name[task] for task in multi_tasks]\n",
    "\n",
    "def get_data(awmids, multi_tasks = [1, 2, 3, 12, 14], prefix='', category='6'):\n",
    "    multi_data = {}\n",
    "    # multi_data: [Dict[aweme_id, Dict[task, List[txt, *info]]]]\n",
    "    ids = ','.join(awmids)\n",
    "    for task in multi_tasks:\n",
    "        sql = data_sql[task].format(version=prefix, category=category, ids=ids)\n",
    "        if task == 11:\n",
    "            dataa = db_sop.query_all(sql, print_sql=False)\n",
    "        else:\n",
    "            dataa = dy2.query_all(sql)\n",
    "        for aweme_id, *info in dataa:\n",
    "            if multi_data.get(aweme_id) is None:\n",
    "                multi_data[aweme_id] = {task: [] for task in multi_tasks}\n",
    "            multi_data[aweme_id][task].append(info)\n",
    "\n",
    "    return multi_data\n",
    "\n",
    "def get_data_source(dy2, cat, prefix=''):\n",
    "    tblnm = f\"douyin_video_zl{prefix}_{cat}\"\n",
    "    sql = f\"select data_source from {project_table} where table_name='{tblnm}';\"\n",
    "    rr = dy2.query_all(sql)\n",
    "    return rr[0][0] if rr[0][0] else None\n",
    "\n",
    "def get_project_cat(dy2, category, prefix):\n",
    "    tbl_nm = f\"douyin_video_zl{prefix}_{category}\"\n",
    "    sql = f'''\n",
    "    select brand, prop from douyin2_cleaner.project\n",
    "    where table_name='{tbl_nm}'\n",
    "    '''\n",
    "    rr = dy2.query_all(sql)\n",
    "    if not rr:\n",
    "        return category, category\n",
    "    else:\n",
    "        brand = rr[0][0] if rr[0][0] else category\n",
    "        prop = rr[0][1] if rr[0][1] else category\n",
    "        return brand, prop\n",
    "\n",
    "brand_cat, prop_cat = get_project_cat(dy2, cat, '')\n",
    "\n",
    "data_type = get_data_source(dy2, cat)\n",
    "print('data_type', data_type)\n",
    "if data_type in (2, 3):\n",
    "    multi_tasks = [1,2,3]\n",
    "    data_sql.update(xiaohongshu_data_sql)\n",
    "elif data_type == 4:\n",
    "    multi_tasks = [1,2,3]\n",
    "    data_sql.update(juliang_data_sql)\n",
    "\n",
    "multi_data = get_data(awmids, multi_tasks = multi_tasks, category=cat)\n",
    "\n",
    "res = []\n",
    "for aweme_id, all_data in multi_data.items():\n",
    "    tmp = ['_'+str(aweme_id), ]\n",
    "    for task, data_list in all_data.items():\n",
    "        txt_ = ','.join([text_normalize(txt) for txt, *info in data_list])\n",
    "        tmp.append(txt_)\n",
    "    res.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "559454be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤规则，质量低的文本不过gpt\n",
    "def count_chars(s):\n",
    "    chinese_chars = 0\n",
    "    english_chars = 0\n",
    "    if not s:\n",
    "        raise ValueError(\"error\")\n",
    "\n",
    "    for char in s:\n",
    "        if char.isalpha():\n",
    "            if char.isascii():\n",
    "                english_chars += 1\n",
    "            else:\n",
    "                chinese_chars += 1\n",
    "\n",
    "    return chinese_chars/len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00850393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 12539.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# 生成对应文本\n",
    "ress = []\n",
    "for awm, tt, xf,ocr,whp in tqdm(res):\n",
    "    if whp and count_chars(whp)>0.5 and len(whp)>20:\n",
    "        ress.append((awm, whp))\n",
    "    elif ocr and count_chars(ocr)>0.5 and len(ocr)>20:\n",
    "        ress.append((awm, ocr))\n",
    "    elif xf and count_chars(xf)>0.5 and len(xf)>20:\n",
    "        ress.append((awm, xf))\n",
    "    if tt and count_chars(tt)>0.5 and len(tt)>20:\n",
    "        ress.append((awm, tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9c05c69",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('_7200307004185873703',\n",
       " '生活不易,被理解也是一种幸福。#711便利店#暖心饮系列 #给生活加点甜 #记录真实生活 #粤语 711便利店;暖心饮系列;给生活加点甜;记录真实生活;粤语')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ress[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt messages生成\n",
    "h_tt_mess = dict()\n",
    "messages = []\n",
    "for ii in ress:\n",
    "    for txtt in ii[1:]:\n",
    "        promptt = str(prompt_head)\n",
    "        prompthead = ast.literal_eval(promptt)\n",
    "        prompthead[1]['content'] = prompthead[1]['content'].format(category_nm=categorynm)\n",
    "        prompthead[3]['content'] = prompthead[3]['content'].format(category_nm=categorynm,txt=txtt)\n",
    "        mm = str(prompthead)\n",
    "        h_tt_mess[txtt] = mm\n",
    "        messages.append(prompthead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7652eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ask_result_dict3 = await gpt_utils.get_chat_answers_nint(messages, mode='nintjp', model='gpt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c68c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e264d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt结果映射\n",
    "resss = []\n",
    "# 这里和prompt head强相关\n",
    "ccp = f\"此文本与{categorynm}相关\"\n",
    "ccn = f\"此文本与{categorynm}无关\"\n",
    "\n",
    "for ii in tqdm(ress):\n",
    "    awm = ii[0][1:]\n",
    "    for txtt in ii[1:]:\n",
    "        if len(txtt)<20:\n",
    "            continue\n",
    "        mm = h_tt_mess[txtt]\n",
    "        response = ask_result_dict3[mm]\n",
    "        try:\n",
    "            resp_detail = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            if ccn in resp_detail:\n",
    "                gpt = '0'\n",
    "            elif ccp in resp_detail:\n",
    "                gpt = '1'\n",
    "                if not txtt:\n",
    "                    continue\n",
    "            resss.append([ii[0], txtt, resp_detail, gpt])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbeb5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据生成\n",
    "train_data_path = f'./data/{cat}/train.txt'\n",
    "dev_data_path = f'./data/{cat}/dev.txt'\n",
    "label_path = f'./data/{cat}/label.txt'\n",
    "\n",
    "resone = []\n",
    "reszero = []\n",
    "for ii in resss:\n",
    "    if ii[-1]=='0':\n",
    "        reszero.append(ii[1])\n",
    "    elif ii[-1]=='1':\n",
    "        resone.append(ii[1])\n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_one, test_one = train_test_split(resone, test_size=0.2, random_state=2023)\n",
    "train_zero, test_zero = train_test_split(reszero, test_size=0.2, random_state=2023)\n",
    "\n",
    "# 打开文件，以写入模式写入内容\n",
    "with open(train_data_path, 'w', encoding='utf-8') as file:\n",
    "    # 将列表元素连接成字符串，使用制表符分隔\n",
    "    for tt in train_one:\n",
    "        # 写入字符串到文件\n",
    "        file.write(tt+'\\t'+f'{categorynm}'+'\\n')\n",
    "    for tt in train_zero:\n",
    "        # 写入字符串到文件\n",
    "        file.write(tt+'\\t'+f'不是{categorynm}'+'\\n')\n",
    "\n",
    "# 打开文件，以写入模式写入内容\n",
    "with open(dev_data_path, 'w', encoding='utf-8') as file:\n",
    "    # 将列表元素连接成字符串，使用制表符分隔\n",
    "    for tt in test_one:\n",
    "        # 写入字符串到文件\n",
    "        file.write(tt+'\\t'+f'{categorynm}'+'\\n')\n",
    "    for tt in test_zero:\n",
    "        # 写入字符串到文件\n",
    "        file.write(tt+'\\t'+f'不是{categorynm}'+'\\n')\n",
    "        \n",
    "# 打开文件，以写入模式写入内容\n",
    "with open(label_path, 'w', encoding='utf-8') as file:\n",
    "    # 将列表元素连接成字符串，使用制表符分隔\n",
    "    file.write(f'{categorynm}'+'\\n')\n",
    "    file.write(f'不是{categorynm}'+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7883c877",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-12-12 16:22:26,329] [    INFO]\u001b[0m - The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m -      Model Configuration Arguments      \u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - paddle commit id              :3a1b1659a405a044ce806fbe027cc146f1193e6d\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - export_model_dir              :None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - model_name_or_path            :ernie-3.0-tiny-medium-v2-zh\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - \u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m -       Data Configuration Arguments      \u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - paddle commit id              :3a1b1659a405a044ce806fbe027cc146f1193e6d\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - bad_case_path                 :./data/bad_case.txt\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - debug                         :False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - dev_path                      :./data/3000003/dev.txt\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - early_stopping                :True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,330] [    INFO]\u001b[0m - early_stopping_patience       :5\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,331] [    INFO]\u001b[0m - label_path                    :./data/3000003/label.txt\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,331] [    INFO]\u001b[0m - max_length                    :512\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,331] [    INFO]\u001b[0m - test_path                     :./data/dev.txt\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,331] [    INFO]\u001b[0m - train_path                    :./data/3000003/train.txt\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,331] [    INFO]\u001b[0m - \u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,331] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-tiny-medium-v2-zh'.\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,331] [    INFO]\u001b[0m - Already cached /home/he.jianshu/.paddlenlp/models/ernie-3.0-tiny-medium-v2-zh/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,332] [    INFO]\u001b[0m - Loading weights file model_state.pdparams from cache at /home/he.jianshu/.paddlenlp/models/ernie-3.0-tiny-medium-v2-zh/model_state.pdparams\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:26,582] [    INFO]\u001b[0m - Loaded weights file from disk, setting weights to model.\u001b[0m\n",
      "W1212 16:22:26.584784 447987 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.2, Runtime API Version: 11.2\n",
      "W1212 16:22:26.586758 447987 gpu_resources.cc:149] device: 0, cuDNN Version: 8.1.\n",
      "\u001b[32m[2023-12-12 16:22:28,145] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing ErnieForSequenceClassification.\n",
      "\u001b[0m\n",
      "\u001b[33m[2023-12-12 16:22:28,145] [ WARNING]\u001b[0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-3.0-tiny-medium-v2-zh and are newly initialized: ['classifier.weight', 'ernie.pooler.dense.bias', 'ernie.pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,150] [    INFO]\u001b[0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'ernie-3.0-tiny-medium-v2-zh'.\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,150] [    INFO]\u001b[0m - Already cached /home/he.jianshu/.paddlenlp/models/ernie-3.0-tiny-medium-v2-zh/ernie_3.0_tiny_medium_v2_vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,181] [    INFO]\u001b[0m - tokenizer config file saved in /home/he.jianshu/.paddlenlp/models/ernie-3.0-tiny-medium-v2-zh/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,182] [    INFO]\u001b[0m - Special tokens file saved in /home/he.jianshu/.paddlenlp/models/ernie-3.0-tiny-medium-v2-zh/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,272] [    INFO]\u001b[0m - ============================================================\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m -     Training Configuration Arguments    \u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - paddle commit id              : 3a1b1659a405a044ce806fbe027cc146f1193e6d\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - _no_sync_in_gradient_accumulation: True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - activation_quantize_type      : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - adam_beta1                    : 0.9\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - adam_beta2                    : 0.999\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - adam_epsilon                  : 1e-08\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - algo_list                     : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - amp_custom_black_list         : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - amp_custom_white_list         : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - amp_master_grad               : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - batch_num_list                : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - batch_size_list               : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,273] [    INFO]\u001b[0m - bf16                          : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - bf16_full_eval                : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - bias_correction               : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - current_device                : gpu:0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - data_parallel_rank            : 0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - dataloader_drop_last          : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - dataloader_num_workers        : 0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - dataset_rank                  : 0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - dataset_world_size            : 1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - device                        : gpu\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - disable_tqdm                  : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - distributed_dataloader        : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - do_compress                   : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - do_eval                       : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - do_export                     : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,274] [    INFO]\u001b[0m - do_predict                    : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - do_train                      : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - eval_accumulation_steps       : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - eval_batch_size               : 32\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - eval_steps                    : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - evaluation_strategy           : IntervalStrategy.EPOCH\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - flatten_param_grads           : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - fp16                          : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - fp16_full_eval                : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - fp16_opt_level                : O1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - gradient_accumulation_steps   : 1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - greater_is_better             : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - hybrid_parallel_topo_order    : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - ignore_data_skip              : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - input_dtype                   : int64\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - input_infer_model_path        : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,275] [    INFO]\u001b[0m - label_names                   : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - lazy_data_processing          : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - learning_rate                 : 3e-05\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - load_best_model_at_end        : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - load_sharded_model            : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - local_process_index           : 0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - local_rank                    : -1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - log_level                     : -1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - log_level_replica             : -1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - log_on_each_node              : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - logging_dir                   : models/3000003/runs/Dec12_16-22-26_nint-MS-7B09\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - logging_first_step            : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - logging_steps                 : 5\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - logging_strategy              : IntervalStrategy.STEPS\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - lr_end                        : 1e-07\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,276] [    INFO]\u001b[0m - lr_scheduler_type             : SchedulerType.LINEAR\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - max_evaluate_steps            : -1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - max_grad_norm                 : 1.0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - max_steps                     : -1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - metric_for_best_model         : accuracy\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - minimum_eval_times            : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - moving_rate                   : 0.9\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - no_cuda                       : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - num_cycles                    : 0.5\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - num_train_epochs              : 10.0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - onnx_format                   : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - optim                         : OptimizerNames.ADAMW\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - optimizer_name_suffix         : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - output_dir                    : models/3000003\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - overwrite_output_dir          : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - past_index                    : -1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,277] [    INFO]\u001b[0m - per_device_eval_batch_size    : 32\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - per_device_train_batch_size   : 32\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - pipeline_parallel_config      : \u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - pipeline_parallel_degree      : -1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - pipeline_parallel_rank        : 0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - power                         : 1.0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - prediction_loss_only          : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - process_index                 : 0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - prune_embeddings              : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - recompute                     : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - remove_unused_columns         : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - report_to                     : ['visualdl']\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - resume_from_checkpoint        : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - round_type                    : round\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - run_name                      : models/3000003\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - save_on_each_node             : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - save_sharded_model            : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,278] [    INFO]\u001b[0m - save_steps                    : 100\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - save_strategy                 : IntervalStrategy.EPOCH\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - save_total_limit              : 1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - scale_loss                    : 32768\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - seed                          : 42\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - sharding                      : []\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - sharding_degree               : -1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - sharding_parallel_config      : \u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - sharding_parallel_degree      : -1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - sharding_parallel_rank        : 0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - should_load_dataset           : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - should_load_sharding_stage1_model: False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - should_log                    : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - should_save                   : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - should_save_model_state       : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,279] [    INFO]\u001b[0m - should_save_sharding_stage1_model: False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - skip_memory_metrics           : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - skip_profile_timer            : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - strategy                      : dynabert+ptq\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - tensor_parallel_config        : \u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - tensor_parallel_degree        : -1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - tensor_parallel_rank          : 0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - train_batch_size              : 32\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - use_hybrid_parallel           : False\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - use_pact                      : True\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - warmup_ratio                  : 0.1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - warmup_steps                  : 0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - weight_decay                  : 0.0\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - weight_name_suffix            : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - weight_quantize_type          : channel_wise_abs_max\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - width_mult_list               : None\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,280] [    INFO]\u001b[0m - world_size                    : 1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,281] [    INFO]\u001b[0m - \u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,281] [    INFO]\u001b[0m - ***** Running training *****\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,282] [    INFO]\u001b[0m -   Num examples = 5,446\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,282] [    INFO]\u001b[0m -   Num Epochs = 10\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,282] [    INFO]\u001b[0m -   Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,282] [    INFO]\u001b[0m -   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,282] [    INFO]\u001b[0m -   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,282] [    INFO]\u001b[0m -   Total optimization steps = 1,710\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,282] [    INFO]\u001b[0m -   Total num train samples = 54,460\u001b[0m\n",
      "\u001b[32m[2023-12-12 16:22:28,283] [    INFO]\u001b[0m -   Number of trainable parameters = 75,416,834 (per device)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1710 [00:00<?, ?it/s]W1212 16:22:29.030934 447987 gpu_resources.cc:275] WARNING: device: \u0000. The installed Paddle is compiled with CUDNN 8.2, but CUDNN version in your machine is 8.1, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.\n",
      "loss: 0.69463539, learning_rate: 8.772e-07, global_step: 5, interval_runtime: 2.5993, interval_samples_per_second: 61.55544275448713, interval_steps_per_second: 1.9236075860777229, epoch: 0.0292\n",
      "loss: 0.6883739, learning_rate: 1.754e-06, global_step: 10, interval_runtime: 1.4299, interval_samples_per_second: 111.89250934412473, interval_steps_per_second: 3.496640917003898, epoch: 0.0585\n",
      "loss: 0.681604, learning_rate: 2.632e-06, global_step: 15, interval_runtime: 1.4415, interval_samples_per_second: 110.99371405378531, interval_steps_per_second: 3.468553564180791, epoch: 0.0877\n",
      "loss: 0.66056333, learning_rate: 3.509e-06, global_step: 20, interval_runtime: 1.4153, interval_samples_per_second: 113.0515593579587, interval_steps_per_second: 3.532861229936209, epoch: 0.117\n",
      "loss: 0.63736095, learning_rate: 4.386e-06, global_step: 25, interval_runtime: 1.4078, interval_samples_per_second: 113.65066300729386, interval_steps_per_second: 3.551583218977933, epoch: 0.1462\n",
      "loss: 0.62513413, learning_rate: 5.263e-06, global_step: 30, interval_runtime: 1.3838, interval_samples_per_second: 115.62186959816972, interval_steps_per_second: 3.613183424942804, epoch: 0.1754\n",
      "loss: 0.57877445, learning_rate: 6.14e-06, global_step: 35, interval_runtime: 1.416, interval_samples_per_second: 112.99618557752956, interval_steps_per_second: 3.5311307992977987, epoch: 0.2047\n",
      "loss: 0.55246811, learning_rate: 7.018e-06, global_step: 40, interval_runtime: 1.4443, interval_samples_per_second: 110.77875930311372, interval_steps_per_second: 3.4618362282223036, epoch: 0.2339\n",
      "loss: 0.54093881, learning_rate: 7.895e-06, global_step: 45, interval_runtime: 1.4186, interval_samples_per_second: 112.7908122756667, interval_steps_per_second: 3.5247128836145842, epoch: 0.2632\n",
      "loss: 0.51339369, learning_rate: 8.772e-06, global_step: 50, interval_runtime: 1.4235, interval_samples_per_second: 112.39850340475586, interval_steps_per_second: 3.5124532313986205, epoch: 0.2924\n",
      "loss: 0.47764044, learning_rate: 9.649e-06, global_step: 55, interval_runtime: 1.4151, interval_samples_per_second: 113.06260634020543, interval_steps_per_second: 3.53320644813142, epoch: 0.3216\n",
      "loss: 0.35783753, learning_rate: 1.053e-05, global_step: 60, interval_runtime: 1.4172, interval_samples_per_second: 112.90012888429871, interval_steps_per_second: 3.5281290276343347, epoch: 0.3509\n",
      "  4%|█▍                                       | 62/1710 [00:18<07:44,  3.55it/s]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 230, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 180, in main\n",
      "    train_result = trainer.train()\n",
      "  File \"/home/he.jianshu/Softwares/anaconda3/envs/tf1nlp/lib/python3.7/site-packages/paddlenlp/trainer/trainer.py\", line 795, in train\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/he.jianshu/Softwares/anaconda3/envs/tf1nlp/lib/python3.7/site-packages/paddlenlp/trainer/trainer.py\", line 1719, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/he.jianshu/Softwares/anaconda3/envs/tf1nlp/lib/python3.7/site-packages/paddlenlp/trainer/trainer.py\", line 1675, in compute_loss\n",
      "    loss = self.criterion(outputs, labels)\n",
      "  File \"/home/he.jianshu/Softwares/anaconda3/envs/tf1nlp/lib/python3.7/site-packages/paddle/nn/layer/layers.py\", line 1254, in __call__\n",
      "    return self.forward(*inputs, **kwargs)\n",
      "  File \"/home/he.jianshu/Softwares/anaconda3/envs/tf1nlp/lib/python3.7/site-packages/paddle/nn/layer/loss.py\", line 382, in forward\n",
      "    name=self.name,\n",
      "  File \"/home/he.jianshu/Softwares/anaconda3/envs/tf1nlp/lib/python3.7/site-packages/paddle/nn/functional/loss.py\", line 2790, in cross_entropy\n",
      "    if paddle.count_nonzero(is_ignore) > 0:  # ignore label\n",
      "  File \"/home/he.jianshu/Softwares/anaconda3/envs/tf1nlp/lib/python3.7/site-packages/paddle/fluid/dygraph/tensor_patch_methods.py\", line 673, in __bool__\n",
      "    return self.__nonzero__()\n",
      "  File \"/home/he.jianshu/Softwares/anaconda3/envs/tf1nlp/lib/python3.7/site-packages/paddle/fluid/dygraph/tensor_patch_methods.py\", line 670, in __nonzero__\n",
      "    return bool(np.array(self) > 0)\n",
      "  File \"/home/he.jianshu/Softwares/anaconda3/envs/tf1nlp/lib/python3.7/site-packages/paddle/fluid/dygraph/tensor_patch_methods.py\", line 696, in __array__\n",
      "    array = self.numpy(False)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# 开始训练模型\n",
    "output_dir = f'models/{cat}'\n",
    "model = \"ernie-3.0-tiny-medium-v2-zh\"\n",
    "\n",
    "! python train.py \\\n",
    "    --train_path $train_data_path\\\n",
    "    --dev_path $dev_data_path\\\n",
    "    --label_path $label_path\\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_export \\\n",
    "    --model_name_or_path $model \\\n",
    "    --output_dir $output_dir \\\n",
    "    --device gpu \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --early_stopping True \\\n",
    "    --early_stopping_patience 5 \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --max_length 512 \\\n",
    "    --per_device_eval_batch_size 32 \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --metric_for_best_model accuracy \\\n",
    "    --load_best_model_at_end \\\n",
    "    --logging_steps 5 \\\n",
    "    --evaluation_strategy epoch \\\n",
    "    --save_strategy epoch \\\n",
    "    --save_total_limit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44039c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0e2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457ed35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
